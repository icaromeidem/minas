{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "366a3737",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/usuario/anaconda3/envs/astrotest/lib/python3.13/site-packages/xgboost/core.py:377: FutureWarning: Your system has an old version of glibc (< 2.28). We will stop supporting Linux distros with glibc older than 2.28 after **May 31, 2025**. Please upgrade to a recent Linux distro (with glibc >= 2.28) to use future versions of XGBoost.\n",
      "Note: You have installed the 'manylinux2014' variant of XGBoost. Certain features such as GPU algorithms or federated learning are not available. To use these features, please upgrade to a recent Linux distro with glibc 2.28+, and install the 'manylinux_2_28' variant.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import astropack2 as ap2\n",
    "from astropack2.tuning._tuner import hyperparameter_search\n",
    "import os\n",
    "from joblib import dump\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7aa234cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caminho do arquivo de entrada\n",
    "input_path = '../data/input_jpasgL.csv'\n",
    "df_base = pd.read_csv(input_path)\n",
    "df_base.set_index('ID', drop=True, inplace=True)\n",
    "\n",
    "# Nome da coluna de índice (ou None para não usar)\n",
    "index_col = 'ID'\n",
    "\n",
    "# Survey e filtros\n",
    "survey_filter = 'JPAS'\n",
    "filters = ap2.FILTERS[survey_filter]\n",
    "\n",
    "# Restrição de erro de magnitude\n",
    "model_rest = '_01'  # '_01' = mais restrito, '_02' = menos restrito\n",
    "\n",
    "# Tipo de modelo: 'RF' para Random Forest, 'XGB' para XGBoost\n",
    "model_type = 'RF'\n",
    "\n",
    "# Ajuste automático do sufixo do survey_train conforme o nome do arquivo de entrada\n",
    "if 'A' in os.path.basename(input_path):\n",
    "    survey_train = f'{survey_filter}A'\n",
    "elif 'L' in os.path.basename(input_path):\n",
    "    survey_train = f'{survey_filter}L'\n",
    "else:\n",
    "    survey_train = survey_filter\n",
    "datetime_str = pd.Timestamp.now().strftime('%Y%m%d%H%M%S')\n",
    "\n",
    "# Lista de parâmetros a serem processados\n",
    "param_list = ['teff', 'logg', 'feh']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1ad8b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parâmetros para tuning (grade reduzida para teste rápido)\n",
    "num_features = 20  # Número de features após pré-processamento\n",
    "k_values = [k for k in [5, num_features] if k <= num_features]\n",
    "\n",
    "param_dist = {\n",
    "    'randomforestregressor__n_estimators': [10, 50],\n",
    "    'randomforestregressor__max_features': [5, num_features],\n",
    "    'randomforestregressor__min_samples_leaf': [1, 5],\n",
    "    'randomforestregressor__bootstrap': [True],\n",
    "    'randomforestregressor__max_depth': [None, 10],\n",
    "    'randomforestregressor__min_samples_split': [2, 10],\n",
    "    'selectkbest__k': k_values\n",
    "}\n",
    "\n",
    "# Parâmetros do RandomizedSearchCV para rodar rápido\n",
    "n_iter = 2\n",
    "cv = 2\n",
    "n_jobs = -1\n",
    "random_state = 42\n",
    "test_size = 0.25\n",
    "save_dir = 'pipeline'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8945756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando processo de criação do dataframe de trabalho:\n",
      "\n",
      "  - Adicionando cores ao dataframe... Tempo: 17.68 s\n",
      "\n",
      "Processo finalizado! Shape da Tabela Gerada: (2060, 1653)\n",
      "Iniciando processo de criação do dataframe de trabalho:\n",
      "\n",
      "  - Adicionando cores ao dataframe... Tempo: 9.08 s\n",
      "\n",
      "Processo finalizado! Shape da Tabela Gerada: (2060, 1653)\n",
      "Iniciando processo de criação do dataframe de trabalho:\n",
      "\n",
      "  - Adicionando cores ao dataframe... Tempo: 9.20 s\n",
      "\n",
      "Processo finalizado! Shape da Tabela Gerada: (2073, 1653)\n"
     ]
    }
   ],
   "source": [
    "# Função utilitária para buscar o nome correto da coluna no DataFrame\n",
    "def get_column(df, aliases):\n",
    "    for col in aliases:\n",
    "        if col in df.columns:\n",
    "            return col\n",
    "    raise KeyError(f'Nenhuma coluna encontrada para: {aliases}')\n",
    "\n",
    "# Aplicar filtros comuns uma vez só\n",
    "df_common = df_base.copy()\n",
    "max_err = 0.1 if model_rest == '_01' else 0.2  # filtro de erro de magnitude\n",
    "error_columns = [f + '_err' for f in filters]\n",
    "\n",
    "# Filtros de erro de magnitude (aplicados uma vez)\n",
    "for col in error_columns:\n",
    "    if col in df_common.columns:\n",
    "        df_common = df_common[df_common[col] <= max_err]\n",
    "\n",
    "# Filtros gerais (aplicados uma vez)\n",
    "df_common = df_common.dropna(subset=['Dist'])\n",
    "\n",
    "\n",
    "# Agora aplicar filtros específicos para cada parâmetro\n",
    "preprocessed = {}\n",
    "for param in param_list:\n",
    "    aliases = ap2.PARAM_ALIASES[param]\n",
    "    df = df_common.copy()\n",
    "    \n",
    "    # Filtros específicos por parâmetro\n",
    "    if param.lower() == 'teff':\n",
    "        param_col = get_column(df, aliases)\n",
    "        df = df[df[param_col] <= 8300]\n",
    "        err_col_candidates = [c for c in df.columns if (c.lower().startswith('e_teff') or c.lower().endswith('teff_err'))]\n",
    "        if err_col_candidates:\n",
    "            err_col = err_col_candidates[0]\n",
    "            df = df[df[err_col] < 300]\n",
    "        df = df[df[param_col] != -9999]\n",
    "        df = df.dropna(subset=[param_col])\n",
    "\n",
    "    elif param.lower() in ['logg']:\n",
    "        param_col = get_column(df, aliases)\n",
    "        err_col_candidates = [c for c in df.columns if (c.lower().startswith('e_'+param.lower()) \n",
    "                                                        or c.lower().endswith(param.lower()+'_err'))]\n",
    "        if err_col_candidates:\n",
    "            err_col = err_col_candidates[0]\n",
    "            df = df[df[err_col] < 0.4]\n",
    "        df = df[df[param_col] != -9999]\n",
    "        df = df.dropna(subset=[param_col])\n",
    "    \n",
    "    elif param.lower() in ['feh']:\n",
    "        param_col = get_column(df, aliases)\n",
    "        err_col_candidates = [c for c in df.columns if (c.lower().startswith('e_'+param.lower()) \n",
    "                                                        or c.lower().endswith(param.lower()+'_err'))]\n",
    "        if err_col_candidates:\n",
    "            err_col = err_col_candidates[0]\n",
    "            df = df[df[err_col] < 0.4]\n",
    "        df = df[df[param_col] != -9999]\n",
    "        df = df.dropna(subset=[param_col])\n",
    "    \n",
    "\n",
    "# Cálculo da magnitude absoluta específico para este parâmetro\n",
    "    df = ap2.preprocess.calculate_abs_mag(df, filters, 'Dist')\n",
    "    \n",
    "    # Criação do dataframe de trabalho específico para este parâmetro\n",
    "    work_df = ap2.preprocess.assemble_work_df(df, \n",
    "                                              filters=filters, \n",
    "                                              correction_pairs=None, \n",
    "                                              add_colors=True, \n",
    "                                              add_combinations=False)\n",
    "    preprocessed[param] = {'df': df, 'work_df': work_df, 'param_col': param_col}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b002e2b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61367a08335f4f05a91bf0248c9a25d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tuning parâmetros:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TUNING_IDs GERADOS:\n",
      "20250923173244_JPASL_teff_01_RF\n",
      "20250923173244_JPASL_logg_01_RF\n",
      "20250923173244_JPASL_feh_01_RF\n"
     ]
    }
   ],
   "source": [
    "# Aplicar tuning para cada parâmetro usando os dados pré-processados\n",
    "tuning_ids = []\n",
    "os.makedirs('pipeline', exist_ok=True)\n",
    "for param, data in tqdm(preprocessed.items(), desc='Tuning parâmetros'):\n",
    "    X = data['work_df']\n",
    "    Y = data['df'][data['param_col']]\n",
    "    tuning_id = f'{datetime_str}_{survey_train}_{param}{model_rest}_{model_type}'\n",
    "    best_pipeline, results = hyperparameter_search(\n",
    "        X, Y,\n",
    "        model_type=model_type,\n",
    "        param_dist=param_dist,\n",
    "        tuning_id=tuning_id,\n",
    "        k_values=k_values,\n",
    "        n_iter=n_iter,\n",
    "        cv=cv,\n",
    "        n_jobs=n_jobs,\n",
    "        random_state=random_state,\n",
    "        save_dir=save_dir\n",
    "    )\n",
    "    dump(best_pipeline, f'pipeline/{tuning_id}_pipeline.joblib')\n",
    "    tuning_ids.append(tuning_id)\n",
    "# Mostrar apenas os tuning_ids ao final\n",
    "print('TUNING_IDs GERADOS:')\n",
    "for tid in tuning_ids:\n",
    "    print(tid)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "astrotest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
